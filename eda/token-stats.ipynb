{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most tokens: 164456\n",
      "Least tokens: 7\n",
      "Average tokens: 4634.21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>header</th>\n",
       "      <th>content</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.gov.uk/government/publications/blu...</td>\n",
       "      <td>Blue Planet Fund</td>\n",
       "      <td>Introduction\\nThe Blue Planet Fund is the UK’s...</td>\n",
       "      <td>2968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.gov.uk/government/publications/blu...</td>\n",
       "      <td>Ocean Country Partnership Programme (OCPP)</td>\n",
       "      <td>The Ocean Country Partnership Programme (OCPP)...</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.gov.uk/government/publications/blu...</td>\n",
       "      <td>Ocean Community Empowerment and Nature (OCEAN)...</td>\n",
       "      <td>The Ocean Community Empowerment and Nature (OC...</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.gov.uk/government/publications/alt...</td>\n",
       "      <td>Alternative ways of managing English fishing q...</td>\n",
       "      <td>Defra committed to considering alternative way...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.gov.uk/government/publications/alt...</td>\n",
       "      <td>Cornwall community quota trial management plan</td>\n",
       "      <td>Group information \\nRoles \\nThe group is a par...</td>\n",
       "      <td>5214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.gov.uk/government/publications/blu...   \n",
       "1  https://www.gov.uk/government/publications/blu...   \n",
       "2  https://www.gov.uk/government/publications/blu...   \n",
       "3  https://www.gov.uk/government/publications/alt...   \n",
       "4  https://www.gov.uk/government/publications/alt...   \n",
       "\n",
       "                                              header  \\\n",
       "0                                   Blue Planet Fund   \n",
       "1         Ocean Country Partnership Programme (OCPP)   \n",
       "2  Ocean Community Empowerment and Nature (OCEAN)...   \n",
       "3  Alternative ways of managing English fishing q...   \n",
       "4     Cornwall community quota trial management plan   \n",
       "\n",
       "                                             content  token_count  \n",
       "0  Introduction\\nThe Blue Planet Fund is the UK’s...         2968  \n",
       "1  The Ocean Country Partnership Programme (OCPP)...          730  \n",
       "2  The Ocean Community Empowerment and Nature (OC...          596  \n",
       "3  Defra committed to considering alternative way...          368  \n",
       "4  Group information \\nRoles \\nThe group is a par...         5214  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# Load the JSON file into a pandas DataFrame\n",
    "# Replace 'your_file.json' with the actual path to your JSON file\n",
    "df = pd.read_json('../govuk-policy-qa-pairs/policy_papers.json')\n",
    "\n",
    "# Combine the 'header' and 'content' into a single feature\n",
    "df['combined'] = df['header'] + \" \" + df['content']\n",
    "\n",
    "# Load the TikToken tokenizer (using GPT-2 as an example; replace with correct tokenizer if needed)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the 'combined' column and count the tokens for each row\n",
    "df['token_count'] = df['combined'].apply(lambda text: len(enc.encode(text)))\n",
    "\n",
    "# Calculate statistics\n",
    "most_tokens = df['token_count'].max()\n",
    "least_tokens = df['token_count'].min()\n",
    "average_tokens = df['token_count'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Most tokens: {most_tokens}\")\n",
    "print(f\"Least tokens: {least_tokens}\")\n",
    "print(f\"Average tokens: {average_tokens:.2f}\")\n",
    "\n",
    "# Optional: Display the DataFrame with token counts\n",
    "df[['url', 'header', 'content', 'token_count']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:  5613\n",
      "Number of documents with more than 10k tokens: 831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing large documents: 100%|██████████| 831/831 [00:05<00:00, 160.36it/s]\n",
      "/tmp/ipykernel_3527450/503322122.py:114: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_combined['split_from_large_doc'] = df_combined['split_from_large_doc'].fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       num_chunks\n",
      "count  831.000000\n",
      "mean     4.590854\n",
      "std      1.851372\n",
      "min      3.000000\n",
      "25%      4.000000\n",
      "50%      4.000000\n",
      "75%      5.000000\n",
      "max     20.000000\n",
      "Most tokens after split: 10078\n",
      "Least tokens after split: 7\n",
      "Average tokens after split: 3172.34\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Load the JSON file into a pandas DataFrame\n",
    "df = pd.read_json('../govuk-policy-qa-pairs/policy_papers.json')\n",
    "\n",
    "print('Size of dataset: ', len(df))\n",
    "\n",
    "# Combine the 'header' and 'content' into a single feature\n",
    "df['combined'] = df['header'] + \" \" + df['content']\n",
    "\n",
    "# Load the TikToken tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the 'combined' column and count the tokens for each row\n",
    "df['token_count'] = df['combined'].apply(lambda text: len(enc.encode(text)))\n",
    "\n",
    "# Find entries with more than 10k tokens\n",
    "threshold = 8000\n",
    "large_docs = df[df['token_count'] > threshold]\n",
    "\n",
    "# List how many documents have more than 10k tokens\n",
    "num_large_docs = len(large_docs)\n",
    "print(f\"Number of documents with more than threshold tokens: {num_large_docs}\")\n",
    "\n",
    "# Function to split a document into chunks of less than 10k tokens, splitting on newlines where possible\n",
    "def split_document_iteratively(text, token_limit=8000, token_overlap=500):\n",
    "    # Tokenize the text\n",
    "    tokens = enc.encode(text)\n",
    "    \n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "\n",
    "    # Iteratively split the document\n",
    "    while start_index < len(tokens):\n",
    "        end_index = min(start_index + token_limit, len(tokens))\n",
    "        \n",
    "        # Try to split at a newline if possible\n",
    "        while end_index > start_index and tokens[end_index - 1] != enc.encode('\\n')[0]:\n",
    "            end_index -= 1\n",
    "        \n",
    "        # If no newline is found, split at the token limit\n",
    "        if end_index == start_index:\n",
    "            end_index = min(start_index + token_limit, len(tokens))\n",
    "        \n",
    "        chunk_tokens = tokens[start_index:end_index]\n",
    "        chunk_text = enc.decode(chunk_tokens)\n",
    "        \n",
    "        # Add notes to indicate the document is split\n",
    "        if start_index > 0:\n",
    "            chunk_text = \"[Note: This is a continuation from the previous part of the document.]\\n\\n\" + chunk_text\n",
    "        \n",
    "        if end_index < len(tokens):\n",
    "            chunk_text += \"\\n\\n[Note: This document was truncated here due to token length limitations. Please see the next part for the continuation.]\"\n",
    "        \n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Move the start index forward with overlap, ensuring it increases\n",
    "        next_start_index = end_index - token_overlap\n",
    "        if next_start_index <= start_index:\n",
    "            start_index = end_index  # Ensure progress\n",
    "        else:\n",
    "            start_index = next_start_index\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Initialize the progress bar for large documents\n",
    "progress_bar = tqdm(total=len(large_docs), desc=\"Processing large documents\")\n",
    "\n",
    "# For tracking stats\n",
    "chunk_stats = []  # To store how many chunks each document is split into\n",
    "new_entries = []\n",
    "processed_docs = 0  # Counter for tracking how many large docs have been processed\n",
    "\n",
    "# For each large document, split it into parts and collect the new entries\n",
    "for _, row in large_docs.iterrows():\n",
    "    # Split the document and create new rows for each part\n",
    "    split_texts = split_document_iteratively(row['combined'])\n",
    "    chunk_stats.append((row['url'], len(split_texts)))  # Track the number of chunks\n",
    "\n",
    "    # Add each part as a new row, ensuring continuation headers where needed\n",
    "    for i, part_text in enumerate(split_texts):\n",
    "        header = row['header'] if i == 0 else row['header'] + \" (continued)\"\n",
    "        new_entries.append({\n",
    "            'url': row['url'],\n",
    "            'header': header,\n",
    "            'content': part_text,\n",
    "            'split_from_large_doc': True  # Mark this row as part of a split document\n",
    "        })\n",
    "\n",
    "    # Update progress bar after processing each document\n",
    "    progress_bar.update(1)\n",
    "    processed_docs += 1\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Create a DataFrame from the new entries\n",
    "new_entries_df = pd.DataFrame(new_entries)\n",
    "\n",
    "# Remove the original large documents from the DataFrame\n",
    "df_small_docs = df[df['token_count'] <= threshold]\n",
    "\n",
    "# Create a separate dataset of just the datapoints less than 10k tokens\n",
    "df_small_docs.to_json('../govuk-policy-qa-pairs/policy_papers_small.json', orient='records', indent=2)\n",
    "\n",
    "# Concatenate the newly split entries back into the main DataFrame\n",
    "df_combined = pd.concat([df_small_docs, new_entries_df], ignore_index=True)\n",
    "\n",
    "# Add a column to mark documents that were not split from a large document\n",
    "df_combined['split_from_large_doc'] = df_combined['split_from_large_doc'].fillna(False)\n",
    "\n",
    "# Recalculate the token counts for the updated dataset\n",
    "df_combined['combined'] = df_combined['header'] + \" \" + df_combined['content']\n",
    "df_combined['token_count'] = df_combined['combined'].apply(lambda text: len(enc.encode(text)))\n",
    "\n",
    "# Save the updated dataset to the specified file path\n",
    "output_file = '../govuk-policy-qa-pairs/policy_papers_truncated.json'\n",
    "df_combined[['url', 'header', 'content', 'split_from_large_doc']].to_json(output_file, orient='records', indent=2)\n",
    "\n",
    "# Display stats about how many chunks each large document was split into\n",
    "chunk_stats_df = pd.DataFrame(chunk_stats, columns=['url', 'num_chunks'])\n",
    "print(chunk_stats_df.describe())  # Print stats like min, max, mean, etc.\n",
    "\n",
    "# Perform the token length analysis again\n",
    "most_tokens = df_combined['token_count'].max()\n",
    "least_tokens = df_combined['token_count'].min()\n",
    "average_tokens = df_combined['token_count'].mean()\n",
    "\n",
    "# Display the updated token statistics\n",
    "print(f\"Most tokens after split: {most_tokens}\")\n",
    "print(f\"Least tokens after split: {least_tokens}\")\n",
    "print(f\"Average tokens after split: {average_tokens:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>header</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.gov.uk/government/publications/blu...</td>\n",
       "      <td>Blue Planet Fund</td>\n",
       "      <td>Introduction\\nThe Blue Planet Fund is the UK’s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.gov.uk/government/publications/blu...</td>\n",
       "      <td>Ocean Country Partnership Programme (OCPP)</td>\n",
       "      <td>The Ocean Country Partnership Programme (OCPP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.gov.uk/government/publications/blu...</td>\n",
       "      <td>Ocean Community Empowerment and Nature (OCEAN)...</td>\n",
       "      <td>The Ocean Community Empowerment and Nature (OC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.gov.uk/government/publications/alt...</td>\n",
       "      <td>Alternative ways of managing English fishing q...</td>\n",
       "      <td>Defra committed to considering alternative way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.gov.uk/government/publications/alt...</td>\n",
       "      <td>Cornwall community quota trial management plan</td>\n",
       "      <td>Group information \\nRoles \\nThe group is a par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.gov.uk/government/publications/blu...   \n",
       "1  https://www.gov.uk/government/publications/blu...   \n",
       "2  https://www.gov.uk/government/publications/blu...   \n",
       "3  https://www.gov.uk/government/publications/alt...   \n",
       "4  https://www.gov.uk/government/publications/alt...   \n",
       "\n",
       "                                              header  \\\n",
       "0                                   Blue Planet Fund   \n",
       "1         Ocean Country Partnership Programme (OCPP)   \n",
       "2  Ocean Community Empowerment and Nature (OCEAN)...   \n",
       "3  Alternative ways of managing English fishing q...   \n",
       "4     Cornwall community quota trial management plan   \n",
       "\n",
       "                                             content  \n",
       "0  Introduction\\nThe Blue Planet Fund is the UK’s...  \n",
       "1  The Ocean Country Partnership Programme (OCPP)...  \n",
       "2  The Ocean Community Empowerment and Nature (OC...  \n",
       "3  Defra committed to considering alternative way...  \n",
       "4  Group information \\nRoles \\nThe group is a par...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file = '../govuk-policy-qa-pairs/policy_papers_truncated.json'\n",
    "df_truncated = pd.read_json(output_file)\n",
    "\n",
    "# Output the head of the DataFrame\n",
    "df_truncated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jargonaut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
